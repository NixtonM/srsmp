# -*- coding: utf-8 -*-
"""Spectroscopy_RH_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193V90d4Vrk0UEfYmDh-wG8wAhOY8GAkj

Import of packages

---

- Edit-> Notebook Setting -> GPU

- Ich habe noch ein Problem mit den Längen der Matrizen, daher [1:] bei einigen Befehle
- Ich  finde es komisch, dass teilweise die Prozentzahl bei der Validation erscheint.
"""

import os 
import numpy as np
import pandas  as pd
import copy
import csv
import glob
import os
import re
import time

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.signal
from sklearn import datasets, linear_model, metrics, mixture, svm, tree
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import ShuffleSplit  # or StratifiedShuffleSplit
from sklearn.model_selection import (GridSearchCV, KFold, cross_val_score,
                                     train_test_split)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, StandardScaler,
                                   scale)
from sklearn.svm import LinearSVC
from sklearn.utils.multiclass import unique_labels
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing

os.listdir()

"""Import of files from gdrive"""

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/open?id=1n6OrazBKoJdw0bEUOYrhf_2KPxi-EWcd'

fluff, id = link.split('=')
print (id) # Verify that you have everything after '='

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('Filename.csv')  
df3 = pd.read_csv('Filename.csv', delimiter = ',', usecols= range(1800))
# Dataset is now stored in a Pandas Dataframe

sol = pd.read_csv('Filename.csv', header=None, delimiter=',', usecols=[1800])

"""Change labels"""

labelencoder = preprocessing.LabelEncoder()
test= labelencoder.fit(sol.values.ravel())
labelSol = test.transform(sol.values.ravel())
plt.hist(labelSol)

df3.shape

"""Import keras"""

from keras.models import Sequential
from keras.layers import Dense
import numpy
# fix random seed for reproducibility
numpy.random.seed(7)
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
import numpy
# fix random seed for reproducibility
numpy.random.seed(7)

# create model
model = Sequential()
model.add(Dense(400, input_dim=1800, activation='relu'))
model.add(Dropout(0.5, noise_shape=None, seed=None))
model.add(Dense(200, activation = "relu"))
model.add(Dropout(0.5, noise_shape=None, seed=None))
model.add(Dense(100, activation = "relu"))
model.add(Dense(6, activation='relu'))
#model.add(Dense(1, activation='softmax'))
model.summary()

model.compile(loss="binary_crossentropy", optimizer='adam', metrics=['accuracy'])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from keras.utils import to_categorical
train_y_2 = to_categorical(labelSol[1:])

model.fit(df3.values, train_y_2 ,epochs=50, batch_size=200)

scores = model.evaluate(df3.values,  train_y_2)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

"""Load the validation set"""

# Validation
linkVal = 'https://drive.google.com/open?id=16d9X9kqTZuCftuHjzWCwJe5B1yqUCDPH'
fluff2, id2 = linkVal.split('=')
print (id2) # Verify that you have everything after '='

downloaded = drive.CreateFile({'id':id2}) 
downloaded.GetContentFile('Filename.csv')  
validation = pd.read_csv('Filename.csv', delimiter = ',', usecols= range(1800))
# Dataset is now stored in a Pandas Dataframe

ValSol = pd.read_csv('Filename.csv', header=None, delimiter=',', usecols=[1800])

labelencoder = preprocessing.LabelEncoder()
test= labelencoder.fit(ValSol.values.ravel())
labelVal = test.transform(ValSol.values.ravel())
plt.hist(labelSol)

#Rearange the target vector
from keras.utils import to_categorical
categorialVal = to_categorical(labelVal[1:])
print(len(categorialVal))

scores = model.evaluate(validation.values,  categorialVal)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

#Normalize
from sklearn.preprocessing import normalize
TrainNorm = normalize(df3.values, axis=1, norm='l2')
ValNorm = normalize(validation.values, axis = 1, norm= 'l2')

from keras.layers import Conv2D, MaxPool2D
# create model
model = Sequential()
model.add(Dense(400, input_dim=1800, activation='relu'))
#model.add(Conv1D(10, 5, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))
model.add(Dropout(0.2, noise_shape=None, seed=None))
#model.add(MaxPool2D())
model.add(Dense(200, activation = "relu"))
model.add(Dropout(0.5, noise_shape=None, seed=None))
model.add(Dense(100, activation = "sigmoid"))
model.add(Dense(6, activation='softmax'))
#model.add(Dense(1, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(df3.values, train_y_2 ,epochs=50, batch_size=300)

scores = model.evaluate(validation.values,  categorialVal)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

# create model
model = Sequential()
model.add(Dense(6, input_dim=1800, activation='relu'))
#model.add(Conv1D(10, 5, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))
model.add(Dropout(0.1, noise_shape=None, seed=None))
#model.add(MaxPool2D())
model.add(Dense(30, activation = "relu"))
model.add(Dropout(0.1, noise_shape=None, seed=None))
model.add(Dense(20, activation = "sigmoid"))
model.add(Dense(6, activation='softmax'))
#model.add(Dense(1, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(df3.values, train_y_2 ,epochs=50, batch_size=300)

scores = model.evaluate(validation.values,  categorialVal)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

model.fit(validation.values,  categorialVal ,epochs=50, batch_size=300)

scores = model.evaluate(df3.values, train_y_2)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

df3.values.shape
from keras import regularizers
print(validation.shape)
# import regularizer
from keras.regularizers import l2
print(len(categorialVal))

from keras.callbacks import EarlyStopping
training_score = []
val_score =[]
test_size = [0.1,0.15,0.2,0.25, 0.3,0.5,0.75,0.98]
losses = ['categorical_crossentropy','binary_crossentropy','kullback_leibler_divergence','poisson']


es = EarlyStopping(monitor='val_categorical_accuracy', verbose=1,patience=15,restore_best_weights=True)

X_test1,X_val, y_test1,y_val = train_test_split(validation.values, categorialVal,train_size=0.1,random_state=42,shuffle =True)
print(X_test1.shape
     )

for loss in losses[0:1]:
    print(loss)
    for size in test_size[0:4]:
    # create model
        print(size)
        X_train, X_test, y_train, y_test = train_test_split(df3.values,
                                                        train_y_2,
                                                      
                                                         train_size = size,
                                                        random_state=42,shuffle = True)
        print(X_train.shape)
        model = Sequential()
        model.add(Dense(5, input_dim=1800, activation='relu',kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l1(0.1)))
        model.add(Dropout(0.3, noise_shape=None, seed=None))
        model.add(Dense(1000, activation='relu'))
        model.add(Dropout(0.5, noise_shape=None, seed=None))
        model.add(Dense(500, activation='sigmoid'))
        model.add(Dense(6, activation='softmax'))
        #model.add(Dense(1, activation='softmax'))
        #model.summary()
        model.compile(loss=loss, optimizer='adam', metrics=['categorical_accuracy'])
        history = model.fit(X_train,y_train  ,epochs=200, batch_size=600, validation_data=(X_test1,y_test1),callbacks=[es])
        scores = model.evaluate(X_train,   y_train)
        training_score.append(scores[1]*100)
        print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
        scores2 = model.evaluate(X_val,  y_val)
        print("\n%s: %.2f%%" % (model.metrics_names[1], scores2[1]*100))
        val_score.append(scores2[1]*100)

print(training_score)
print(val_score)

trainingData = np.asarray(training_score).reshape((-1,len(test_size)))
trainingData.shape
ValidationData = np.asarray(val_score).reshape((-1,len(test_size)))
ValidationData.shape

colors = ['green','orange','brown','pink']
f,ax = plt.subplots(figsize=(20/2.54,15/2.54))
for i,(loss,row) in enumerate(zip(losses,trainingData)) :
    print(row)
    plt.plot(test_size,row, label = 'Training: ' + loss ,color = colors[i],ls='-')
for i,(loss,row) in enumerate(zip(losses,ValidationData)) :
    print(row)
    plt.plot(test_size,row,label = 'Validation: ' + loss,color = colors[i],ls='--')
plt.legend()
plt.ylabel('Accurancy [%]')
plt.xlabel('Test size [-]')
plt.show()

#Normalize
from sklearn.preprocessing import normalize
TrainNorm = normalize(df3.values, axis=1, norm='l2')
ValNorm = normalize(validation.values, axis = 1, norm= 'l2')


from keras.callbacks import EarlyStopping
training_score = []
val_score =[]
test_size = [0.1,0.15,0.2,0.25, 0.3,0.5,0.75,0.98]
losses = ['categorical_crossentropy','binary_crossentropy','kullback_leibler_divergence','poisson']


es = EarlyStopping(monitor='val_categorical_accuracy', verbose=1,patience=15,restore_best_weights=True)

X_test1,X_val, y_test1,y_val = train_test_split(ValNorm, categorialVal,train_size=0.1,random_state=7,shuffle =True)
print(X_test1.shape
     )

for loss in losses:
    print(loss)
    for size in test_size[::2]:
    # create model
        print(size)
        X_train, X_test, y_train, y_test = train_test_split(TrainNorm,
                                                        train_y_2,
                                                      
                                                         train_size = size,
                                                        random_state=42,shuffle = True)
        print(X_train.shape)
        model = Sequential()
        model.add(Dense(5, input_dim=1800, activation='relu',kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l1(0.1)))
        #model.add(Dropout(0.3, noise_shape=None, seed=None))
        model.add(Dense(1000, activation='relu'))
        #model.add(Dropout(0.5, noise_shape=None, seed=None))
        model.add(Dense(10, activation='relu'))
        model.add(Dense(6, activation='softmax'))
        #model.add(Dense(1, activation='softmax'))
        model.summary()
        model.compile(loss=loss, optimizer='adamax', metrics=['categorical_accuracy'])
        history = model.fit(X_train,y_train  ,epochs=200, batch_size=600, validation_data=(X_test1,y_test1),callbacks=[es])
        scores = model.evaluate(X_train,   y_train)
        training_score.append(scores[1]*100)
        print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
        scores2 = model.evaluate(X_val,  y_val)
        print("\n%s: %.2f%%" % (model.metrics_names[1], scores2[1]*100))
        val_score.append(scores2[1]*100)

print(training_score)
print(val_score)

# create model
model = Sequential()
model.add(Dense(3, input_dim=1800, activation='relu'))
#model.add(Conv1D(10, 5, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))
model.add(Dropout(0.1, noise_shape=None, seed=None))
#model.add(MaxPool2D())
model.add(Dense(100, activation = "relu"))
model.add(Dropout(0.1, noise_shape=None, seed=None))
model.add(Dense(10, activation = "relu"))
model.add(Dense(6, activation='softmax'))
#model.add(Dense(1, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(df3.values, train_y_2 ,epochs=50, batch_size=600)
scores = model.evaluate(validation.values,  categorialVal)
print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))



train_y_2[1:-1:500,:]

scores2 = model.evaluate(validation.values[::-1],  categorialVal[::-1], batch_size=100) # Attention wront labgels to test
print("\n%s: %.2f%%" % (model.metrics_names[1], scores2[1]*100))

from sklearn.metrics import accuracy_score

y_pred = model.predict(validation.values[::-1]) #Attention wrong solution
acc = sum([np.argmax(train_y_2[i])==np.argmax(y_pred[i]) for i in range(y_pred.shape[0])])/y_pred.shape[0]
acc

accuracy_score(model.model.predict_classes(validation.values),categorialVal)

model.model.predict_classes(validation.values[1:10,:])

categorialVal[1:10]



"""Python Code für Klassifikation 
- Sklearn wird nicht beschleunigt durch GPU.
"""

import os 
import numpy as np
import pandas  as pd
import copy
import csv
import glob
import os
import re
import time

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.signal
from sklearn import datasets, linear_model, metrics, mixture, svm, tree
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import ShuffleSplit  # or StratifiedShuffleSplit
from sklearn.model_selection import (GridSearchCV, KFold, cross_val_score,
                                     train_test_split)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, StandardScaler,
                                   scale)
from sklearn.svm import LinearSVC
from sklearn.utils.multiclass import unique_labels

def fit_classfier(clfs, namen, scaler_name, X_train, Y_train, X_test, Y_test):
    results  = np.zeros((len(clfs),len(scaler_name)))
    for nrClf, clf in enumerate(clfs):
        print(clf)
        pipe1 = make_pipeline(clf)
        pipe2 = make_pipeline(StandardScaler(), clf)
        pipe3 = make_pipeline(MaxAbsScaler(),clf)
        pipe4 = make_pipeline(MinMaxScaler(),clf)
        pipe5 = make_pipeline(Normalizer(norm='l1'),clf)
        pipe6 = make_pipeline(Normalizer(),clf)
        pipes = [pipe1, pipe2, pipe3, pipe4,pipe5,pipe6]
        for nr,pip in enumerate(pipes):
            print(scaler_name[nr])
            pip.fit(X_train,Y_train)
            sol = pip.predict(X_test)
            print(accuracy_score(Y_test, pip.predict(X_test)))
            results[nrClf,nr] = accuracy_score(Y_test, pip.predict(X_test))
    return results

MSol = fit_classfier(clfs,namen,scaler_name,df3.values, categorialVal,validation.values,  categorialVal)
for row in range(MSol.shape[0]):
    plt.plot(MSol[row,:], label = namen[row], marker = 'x')
    plt.legend()
    plt.xticks(np.arange(len(scaler_name)), (scaler_name), rotation = 90)
    plt.show()

from sklearn import preprocessing
from sklearn.preprocessing import Normalizer

knn = KNeighborsClassifier(n_jobs=-1)
RF = RandomForestClassifier(n_estimators=30,n_jobs=-1)
DTC = tree.DecisionTreeClassifier(max_depth=9, criterion='entropy')
sgd = linear_model.SGDClassifier(max_iter=1000, loss='log', penalty='l1' ,shuffle=True,n_jobs=-1)
lda = LinearDiscriminantAnalysis()
LSCV = LinearSVC()

namen = ('kNN', 'Random Forest', 'Desicion Tree', 'SGD', 'LDA', 'LSCV')
clfs = (knn, RF, DTC, sgd, lda, LSCV)
scaler_name = ['No Scaler','StandardScaler','MaxAbsScaler', 'MinMaxScaler', 'Normalize l1','Normalize l2']

